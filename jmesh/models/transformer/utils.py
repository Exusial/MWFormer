import jittor as jt 
from jittor import nn
import math

def erfinv(x):
    CPU_HEADER = r'''
    float erfinv(float y) {
    float x, z, num, dem; /*working variables */
    /* coefficients in rational expansion */
    float a[4] = {  float(0.886226899), float(-1.645349621),  float(0.914624893), float(-0.140543331) };
    float b[4] = { float(-2.118377725),  float(1.442710462), float(-0.329097515),  float(0.012229801) };
    float c[4] = { float(-1.970840454), float(-1.624906493),  float(3.429567803),  float(1.641345311) };
    float d[2] = {  float(3.543889200),  float(1.637067800) };
    float y_abs = std::abs(y);
    if(y_abs > 1.0) return std::numeric_limits<float>::quiet_NaN();
    if(y_abs == 1.0) return std::copysign(std::numeric_limits<float>::infinity(), y);
    if(y_abs <= static_cast<float>(0.7)) {
    z = y * y;
    num = (((a[3]*z + a[2])*z + a[1])*z + a[0]);
    dem = ((((b[3]*z + b[2])*z + b[1])*z +b[0]) * z + static_cast<float>(1.0));
    x = y * num / dem;
    }
    else{
    z = std::sqrt(-std::log((static_cast<float>(1.0)-y_abs)/static_cast<float>(2.0)));
    num = ((c[3]*z + c[2])*z + c[1]) * z + c[0];
    dem = (d[1]*z + d[0])*z + static_cast<float>(1.0);
    x = std::copysign(num, y) / dem;
    }
    /* Two steps of Newton-Raphson correction */
    x = x - (std::erf(x) - y) / ((static_cast<float>(2.0)/static_cast<float>(std::sqrt(M_PI)))*std::exp(-x*x));
    x = x - (std::erf(x) - y) / ((static_cast<float>(2.0)/static_cast<float>(std::sqrt(M_PI)))*std::exp(-x*x));
    return(x);
    }
    '''
    return jt.code(x.shape,x.dtype,[x],
                  cpu_header=CPU_HEADER,
                  cpu_src=r'''for(int i=0;i<in0->num;i++){
                                 out0_p[i]=erfinv(in0_p[i]); 
                              }'''
                  )

def trunc_normal(tensor,  mean=0., std=1., a=-2., b=2.):
    _sqrt2 = 1.4142135623730951
    
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / _sqrt2)) / 2.

    with jt.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        nn.init.uniform_(tensor,2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor = erfinv(tensor)

        # Transform to proper mean, std
        tensor = tensor*std*_sqrt2 + mean

        # Clamp to ensure it's in the proper range
        tensor = jt.clamp(tensor,min_v=a, max_v=b)
        return tensor


def _conv_filter(state_dict, patch_size=16):
    """ convert patch embedding weight from manual patchify + linear proj to conv"""
    out_dict = {}
    for k, v in state_dict.items():
        if 'patch_embed.proj.weight' in k:
            v = v.reshape((v.shape[0], 3, patch_size, patch_size))
        out_dict[k] = v
    return out_dict
    
def load_pretrained(model, cfg=None, num_classes=1000, in_chans=3,filter_fn=None):
    if cfg is None:
        cfg = getattr(model, 'default_cfg')
    if cfg is None or 'url' not in cfg or not cfg['url']:
        return
    from torch.utils import model_zoo
    state_dict = model_zoo.load_url(cfg['url'], progress=False, map_location='cpu')

    if filter_fn is not None:
        state_dict = filter_fn(state_dict)

    classifier_name = cfg['classifier']
    if num_classes == 1000 and cfg['num_classes'] == 1001:
        # special case for imagenet trained models with extra background class in pretrained weights
        classifier_weight = state_dict[classifier_name + '.weight']
        state_dict[classifier_name + '.weight'] = classifier_weight[1:]
        classifier_bias = state_dict[classifier_name + '.bias']
        state_dict[classifier_name + '.bias'] = classifier_bias[1:]
    elif num_classes != cfg['num_classes']:
        # completely discard fully connected for all other differences between pretrained and created model
        del state_dict[classifier_name + '.weight']
        del state_dict[classifier_name + '.bias']

    model.load_parameters(state_dict)



class AverageMeter:
    """Computes and stores the average and current value"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count
        for k, v in self.__dict__.items():
            if isinstance(v, jt.Var):
                v.stop_grad().sync()


def accuracy(output, target, topk=(1,)):
    """Computes the accuracy over the k top predictions for the specified values of k"""
    maxk = max(topk)
    batch_size = target.size(0)
    _, pred = output.topk(maxk, 1, True, True)
    pred = pred.transpose(1,0)
    correct = pred == (target.view(1, -1).expand_as(pred))
    return [correct[:k].view(-1).float().sum(0) * 100. / batch_size for k in topk]
